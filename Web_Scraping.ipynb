{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a023987",
   "metadata": {},
   "source": [
    "# Web Scraping Real Estate Data from MagicBricks\n",
    "\n",
    "This notebook demonstrates how to scrape real estate property listings from MagicBricks.com using Python. We'll use `Selenium` to automate browser interactions and `BeautifulSoup` to parse the HTML content. The extracted data will then be organized into a Pandas DataFrame and saved as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3d627",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment and Dependencies\n",
    "\n",
    "First, we need to import the necessary libraries. If you don't have them installed, you can install them using `pip`:\n",
    "`pip install selenium beautifulsoup4 pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a6a6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ_T021d7_oW",
    "outputId": "97217691-3013-43f1-b969-e3141f021c32"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# selenium: Automates web browser interactions.\n",
    "# BeautifulSoup: Parses HTML content for data extraction.\n",
    "# pandas: Used for data manipulation (DataFrames) and CSV export.\n",
    "# time: Provides a simple way to pause script execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c308d7f",
   "metadata": {},
   "source": [
    "## 2. Initializing the Web Driver and Navigating to the URL\n",
    "\n",
    "This section sets up the Chrome browser for automation, including options to maximize the window and reduce bot detection, then navigates to the target URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa167c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-Zt021d7_oX",
    "outputId": "65b9319e-e325-4c60-8456-5510d9326e1c"
   },
   "outputs": [],
   "source": [
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "# Ensure chromedriver.exe is in your PATH or specify its path\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL for property listings in Delhi\n",
    "url = \"https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment&cityName=Delhi\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load dynamic content\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50453ba3",
   "metadata": {},
   "source": [
    "## 3. Parsing the Page Content and Quitting the Driver\n",
    "\n",
    "After the page loads, its HTML content is retrieved and processed by BeautifulSoup. The browser is then closed to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b43d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-wT021d7_oY",
    "outputId": "36142750-7164-42f8-a111-54c34a942475"
   },
   "outputs": [],
   "source": [
    "# Get the page source and parse with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2fe90",
   "metadata": {},
   "source": [
    "## 4. Extracting Property Data\n",
    "\n",
    "This is the core scraping logic, iterating through each property listing card to extract details. It includes error handling to manage inconsistencies in website structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a836e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_zT021d7_oZ",
    "outputId": "36142750-7164-42f8-a111-54c34a942475"
   },
   "outputs": [],
   "source": [
    "# Find all property cards\n",
    "cards = soup.find_all(\"div\", class_=\"m-srp-card__container\")\n",
    "\n",
    "# List to store extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through each property card\n",
    "for card in cards:\n",
    "    try:\n",
    "        # Extract basic information\n",
    "        name = card.find(\"h2\", class_=\"m-srp-card__title\").get_text(strip=True)\n",
    "        price = card.find(\"div\", class_=\"m-srp-card__price\").get_text(strip=True)\n",
    "        \n",
    "        rate_element = card.find(\"div\", class_=\"m-srp-card__price--size\")\n",
    "        rate = rate_element.get_text(strip=True) if rate_element else \"\"\n",
    "        \n",
    "        location = card.find(\"div\", class_=\"m-srp-card__location\").get_text(strip=True)\n",
    "        \n",
    "        # Extract features\n",
    "        features = card.find_all(\"div\", class_=\"m-srp-card__summary__list__item\")\n",
    "\n",
    "        bedroom = features[0].get_text(strip=True) if len(features) > 0 else \"\"\n",
    "        carpet_area = features[1].get_text(strip=True) if len(features) > 1 else \"\"\n",
    "        status = features[2].get_text(strip=True) if len(features) > 2 else \"\"\n",
    "        floor = features[3].get_text(strip=True) if len(features) > 3 else \"\"\n",
    "\n",
    "        # Extract detailed description parts\n",
    "        desc_block = card.find(\"div\", class_=\"m-srp-card__desc\")\n",
    "        desc_text = desc_block.get_text(separator='|', strip=True) if desc_block else \"\"\n",
    "        desc_parts = desc_text.split(\"|\")\n",
    "\n",
    "        transaction = desc_parts[0] if len(desc_parts) > 0 else \"\"\n",
    "        facing = desc_parts[1] if len(desc_parts) > 1 else \"\"\n",
    "        overlooking = desc_parts[2] if len(desc_parts) > 2 else \"\"\n",
    "        ownership = desc_parts[3] if len(desc_parts) > 3 else \"\"\n",
    "        parking = desc_parts[4] if len(desc_parts) > 4 else \"\"\n",
    "        bathroom = desc_parts[5] if len(desc_parts) > 5 else \"\"\n",
    "        balcony = desc_parts[6] if len(desc_parts) > 6 else \"\"\n",
    "        \n",
    "        city = \"Delhi\"\n",
    "\n",
    "        # Clean numerical data\n",
    "        carpet_area_sqft = ''.join(filter(str.isdigit, carpet_area))\n",
    "        rate_per_sqft = ''.join(filter(str.isdigit, rate))\n",
    "        total_area = \"\"\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            \"Name\": name,\n",
    "            \"Price\": price,\n",
    "            \"Rate\": rate,\n",
    "            \"Property\": name.split('Flat for Sale')[0].strip() if \"Flat for Sale\" in name else \"\",\n",
    "            \"Carpet Area\": carpet_area,\n",
    "            \"Status\": status,\n",
    "            \"Floor\": floor,\n",
    "            \"Transaction\": transaction,\n",
    "            \"Facing\": facing,\n",
    "            \"Overlooking\": overlooking,\n",
    "            \"Ownership\": ownership,\n",
    "            \"Parking\": parking,\n",
    "            \"Bathroom\": bathroom,\n",
    "            \"Balcony\": balcony,\n",
    "            \"City\": city,\n",
    "            \"Location\": location,\n",
    "            \"Rate_per_sqft\": rate_per_sqft,\n",
    "            \"Bedroom\": bedroom.split(\" \")[0] if bedroom else \"\",\n",
    "            \"Carpet_area_sqft\": carpet_area_sqft,\n",
    "            \"Total_area\": total_area\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing property: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90105be7",
   "metadata": {},
   "source": [
    "## 5. Creating a Pandas DataFrame and Saving to CSV\n",
    "\n",
    "Finally, the collected data is converted into a Pandas DataFrame and saved as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086e056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-wT021d7_oa",
    "outputId": "36142750-7164-42f8-a111-54c34a942475"
   },
   "outputs": [],
   "source": [
    "# Convert data list to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(\"magicbricks_full_data.csv\", index=False)\n",
    "\n",
    "print(\"Scraping complete. Data saved to magicbricks_full_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MagicBricks_Web_Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
